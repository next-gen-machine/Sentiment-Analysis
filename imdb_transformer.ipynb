{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ef5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ec71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f6f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Concatenate,Dense,Input,Embedding,Bidirectional,LSTM,GRU\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41c41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(\"/data2/scratch/rohitr/datasets/IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c27a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'negative':0,'positive':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdf7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df['sentiment'] = imdb_df['sentiment'].apply(lambda x : d.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(imdb_df['review'],imdb_df['sentiment'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3818b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4738a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146b5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e52dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfaf4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_text\n",
    "import re\n",
    "def clean_text(text,punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']):\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '',text)\n",
    "    text = re.sub(r'<.*?>', '',text)\n",
    "    # Removing the punctuations\n",
    "    for x in text.lower(): \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\")\n",
    "            \n",
    "    text = text.lower()\n",
    "    # Removing stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    \n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ',text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e991071",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [clean_text(x) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6642508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [clean_text(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ab4705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366c0ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "060988e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "t.fit_on_texts(X_train)\n",
    "text_matrix=t.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "022120df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "101b6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6619c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192305"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88db2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "text_pad = pad_sequences(text_matrix, maxlen=200, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "024b3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pad = pad_sequences(test_matrix, maxlen=200, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad3528cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b253ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21cf5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f300f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9afba326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n",
      "17473536/17464789 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30000\u001b[39m  \u001b[38;5;66;03m# Only consider the top 30k words\u001b[39;00m\n\u001b[1;32m      2\u001b[0m maxlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# Only consider the first 200 words of each movie review\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m (x_train, y_train), (x_val, y_val) \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_train), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_val), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/yes/envs/tf_new/lib/python3.8/site-packages/keras/datasets/imdb.py:109\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m path \u001b[38;5;241m=\u001b[39m get_file(\n\u001b[1;32m    104\u001b[0m     path,\n\u001b[1;32m    105\u001b[0m     origin\u001b[38;5;241m=\u001b[39morigin_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimdb.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    106\u001b[0m     file_hash\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m   x_train, labels_train \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    110\u001b[0m   x_test, labels_test \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test\u001b[39m\u001b[38;5;124m'\u001b[39m], f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    112\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(seed)\n",
      "File \u001b[0;32m~/yes/envs/tf_new/lib/python3.8/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/yes/envs/tf_new/lib/python3.8/site-packages/numpy/lib/format.py:732\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m     pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# Friendlier error message\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickling a python object failed: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to pass the encoding= option \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto numpy.load\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (err,))\n",
      "File \u001b[0;32m~/yes/envs/tf_new/lib/python3.8/zipfile.py:917\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadable\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;124;03m\"\"\"Read and return up to n bytes.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    If the argument is omitted, None, or negative, data is read and returned until EOF is reached.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size = 30000  # Only consider the top 30k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70f7ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 19:24:29.303644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-02 19:24:32.214060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38418 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(200,))\n",
    "embedding_layer = TokenAndPositionEmbedding(200, vocab_length, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1,activation='sigmoid',trainable = True)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c81181e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33fcf36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 19:24:36.847288: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   7/1000 [..............................] - ETA: 21s - loss: 0.6989 - accuracy: 0.5580   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 19:24:46.630879: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 33s 23ms/step - loss: 0.3645 - accuracy: 0.8312 - val_loss: 0.2953 - val_accuracy: 0.8761\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.1538 - accuracy: 0.9443 - val_loss: 0.2841 - val_accuracy: 0.8851\n"
     ]
    }
   ],
   "source": [
    "hist = model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    text_pad, y_train, batch_size=32, epochs=2, validation_split= 0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a7710f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 13ms/step - loss: 0.2924 - accuracy: 0.8866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29236993193626404, 0.8866000175476074]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_pad,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567a405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
